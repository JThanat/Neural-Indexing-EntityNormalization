{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example of how to use Siamese BioBERT to create IR system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import requests\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = '../'\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.model_data_utils import *\n",
    "from utils.annoy_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set default device to cuda if available\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting UP Elastic Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_HOST = '172.17.0.3'\n",
    "ES_PORT = '9200'\n",
    "EVAL_INDEX = 'uniprot_protein'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setting is specific to Novartis machine as we need to connect to local host in this example. Thus, we will remove all env variable for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['http_proxy'] = \"\"\n",
    "os.environ['HTTP_PROXY'] = \"\"\n",
    "os.environ['https_proxy'] = \"\"\n",
    "os.environ['HTTPS_PROXY'] = \"\"\n",
    "os.environ['NO_PROXY'] = \"\"\n",
    "os.environ['no_proxy'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Search Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([ {'host': ES_HOST, 'port': ES_PORT}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Dataset to index\n",
    "In this case we will use reference_data.tsv as an example dataset to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14-3-3 protein beta/alpha</td>\n",
       "      <td>P31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Protein 1054</td>\n",
       "      <td>P31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Protein kinase C inhibitor protein 1</td>\n",
       "      <td>P31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KCIP-1</td>\n",
       "      <td>P31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14-3-3 protein beta/alpha, N-terminally processed</td>\n",
       "      <td>P31946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124465</th>\n",
       "      <td>y(_)L-type amino acid transporter 2</td>\n",
       "      <td>Q92536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124466</th>\n",
       "      <td>y_ system cationic amino acid transporter</td>\n",
       "      <td>Q01650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124467</th>\n",
       "      <td>y_LAT-1</td>\n",
       "      <td>Q9UM01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124468</th>\n",
       "      <td>y_LAT-2</td>\n",
       "      <td>Q92536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124469</th>\n",
       "      <td>yi028</td>\n",
       "      <td>A8MVJ9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124470 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name      id\n",
       "0                               14-3-3 protein beta/alpha  P31946\n",
       "1                                            Protein 1054  P31946\n",
       "2                    Protein kinase C inhibitor protein 1  P31946\n",
       "3                                                  KCIP-1  P31946\n",
       "4       14-3-3 protein beta/alpha, N-terminally processed  P31946\n",
       "...                                                   ...     ...\n",
       "124465                y(_)L-type amino acid transporter 2  Q92536\n",
       "124466          y_ system cationic amino acid transporter  Q01650\n",
       "124467                                            y_LAT-1  Q9UM01\n",
       "124468                                            y_LAT-2  Q92536\n",
       "124469                                              yi028  A8MVJ9\n",
       "\n",
       "[124470 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REFERENCE_DATA_PATH = os.path.join(ROOT_DIR, 'data/reference_data.tsv')\n",
    "reference_copus_df = pd.read_csv(REFERENCE_DATA_PATH, delimiter='\\t')\n",
    "reference_copus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_es_embedding(corpus_df, index='', model=None):\n",
    "    mod_factor = len(corpus_df)//10\n",
    "    for idx, row in corpus_df.iterrows():\n",
    "        if idx % mod_factor == 0 or idx == len(corpus_df)-1:\n",
    "            print(idx*100/len(corpus_df))\n",
    "            \n",
    "        payload = dict()\n",
    "        payload['name'] = row['name'].strip()\n",
    "        payload['qid'] = row['id']\n",
    "\n",
    "        if model:\n",
    "            word_emb = model.encode(row['name'], device=device)\n",
    "            payload['word_embedding'] = word_emb\n",
    "            \n",
    "        try:\n",
    "            res = es.create(index=index, body=payload, id=str(idx))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an index mapping for Elasticsearch\n",
    "Before we start indexing with Elasticsearch, we have to define how we want to index our items first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.create(\n",
    "    index=EVAL_INDEX,\n",
    "    body={\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"name\": {\n",
    "                    \"type\": \"text\"\n",
    "                },\n",
    "                \"qid\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                },\n",
    "                \"word_embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 128 # 768 by default\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    ignore=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = None # Please specify model you want to use here\n",
    "model = get_model(model_name, device=device)\n",
    "index_to_es_embedding(reference_copus_df, index=EVAL_INDEX, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query to Elasticsearch with our embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function to Query ElasticSearch with Fuzzy Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_elastic_search_fuzzy(keyword, **kwargs):\n",
    "    url = \"http://{}:{}/{}/_search\".format(ES_HOST, ES_PORT, EVAL_INDEX)\n",
    "    payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"match\" : {\n",
    "                \"name\" : {\n",
    "                    \"query\" : keyword,\n",
    "                    \"fuzziness\": \"auto\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        top_k = kwargs['top_k']\n",
    "        r = requests.get(url, json = payload)\n",
    "        hits = r.json()['hits']['hits']\n",
    "        hits = [(doc['_source']['name'], doc['_source']['qid']) for doc in hits[:top_k]]\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function to Query Elasticsearch with ElasticSearch Vector Field (Linear Scan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_elastic_search_with_embed(keyword, **kwargs):\n",
    "    if 'model' not in kwargs:\n",
    "        print(\"No Model Found, returning without querying ES\")\n",
    "        return []\n",
    "    model = kwargs['model']\n",
    "    top_k = kwargs['top_k'] if 'top_k' in kwargs else 10\n",
    "    url = \"http://{}:{}/{}/_search\".format(ES_HOST, ES_PORT, EVAL_INDEX)\n",
    "    vector = list(model.encode(str(keyword), device=device).astype(float))\n",
    "    payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\" : {\n",
    "                    \"match_all\" : {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'word_embedding') + 1.0\", \n",
    "                    \"params\": {\n",
    "                        \"query_vector\": vector\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, json = payload)\n",
    "        hits = r.json()['hits']['hits']\n",
    "        hits = [(doc['_source']['name'], doc['_source']['qid']) for doc in hits[:top_k]]\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Function Query Elasticsearch with Vector and Fuzzy Match\n",
    "\n",
    "This function will filter the search result using Fuzzy Match and then rerank them with Embedded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_elastic_search_with_fuzzy_and_embed(keyword, **kwargs):\n",
    "    if 'model' not in kwargs:\n",
    "        print(\"No Model Found, returning without querying ES\")\n",
    "        return []\n",
    "    model = kwargs['model']\n",
    "    top_k = kwargs['top_k'] if 'top_k' in kwargs else 10\n",
    "    url = \"http://{}:{}/{}/_search\".format(ES_HOST, ES_PORT, EVAL_INDEX)\n",
    "    vector = list(model.encode(str(keyword), device=device).astype(float))\n",
    "    payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\" : {\n",
    "                    \"match\" : {\n",
    "                        \"name\" : {\n",
    "                            \"query\" : keyword,\n",
    "                            \"fuzziness\": \"auto\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'word_embedding') + 1.0\", \n",
    "                    \"params\": {\n",
    "                        \"query_vector\": vector\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        r = requests.get(url, json = payload)\n",
    "        hits = r.json()['hits']['hits']\n",
    "        hits = [(doc['_source']['name'], doc['_source']['qid']) for doc in hits[:top_k]]\n",
    "        return hits\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'Protein kinase C'\n",
    "query_elastic_search_fuzzy(search_term, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_elastic_search_with_embed(search_term, model=model, top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_elastic_search_with_fuzzy_and_embed(search_term, model=model, top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annoy Indexing\n",
    "To use Annoy as an indexed reference data, we need to first create annoy file and then issue the query to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-computed embeddings from disc\n"
     ]
    }
   ],
   "source": [
    "model_name = None\n",
    "model = get_model(model_name, device=device)\n",
    "annoy_object_wrapper = AnnoyObjectWrapper(index_path='./protein-embedding-4096-trees.ann', \n",
    "                          embedding_path='./protein-768-embedding.pkl', \n",
    "                          reference_dataset_path=REFERENCE_DATA_PATH, \n",
    "                          name2id_path='./protein-name2id-embedding-size-1500000', \n",
    "                          model=model, n_trees=4096, embedding_size=768, max_corpus_size=1500000)\n",
    "annoy_object_wrapper.create_embedding_and_index(create_new_embedding=True, create_new_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id2name(dataset_path):\n",
    "    all_name = pd.read_csv(dataset_path, delimiter='\\t')\n",
    "    all_name = all_name.dropna() \n",
    "    \n",
    "    id2name = {}\n",
    "    for idx, row in all_name.iterrows():\n",
    "        if row['id'].strip() not in id2name:\n",
    "            id2name[row['id']] = set()\n",
    "        id2name[row['id']].add(row['name'].strip())\n",
    "        \n",
    "    return id2name\n",
    "\n",
    "\n",
    "def get_name2id(dataset_path):\n",
    "    all_name = pd.read_csv(dataset_path, delimiter='\\t')\n",
    "    all_name = all_name.dropna() \n",
    "    \n",
    "    name2id = {}\n",
    "    for idx, row in all_name.iterrows():\n",
    "        if row['name'].strip() not in name2id:\n",
    "            name2id[row['name']] = set()\n",
    "        name2id[row['name']].add(row['id'].strip())\n",
    "        \n",
    "    return name2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name = get_id2name(REFERENCE_DATA_PATH)\n",
    "name2id = get_name2id(REFERENCE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how to query with Annoy Indexing\n",
    "To query with annoy index, we need to supply an extra argument so that we can get the actual id of the reference data. Since Annoy use its own indexing, we cannot use the value return from Annoy as a reference data id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_with_annoy(keyword, **kwargs):\n",
    "    model = kwargs['model']\n",
    "    annoy_object_wrapper = kwargs['annoy_object_wrapper']\n",
    "    id2name = kwargs['id2name']\n",
    "    top_k = kwargs['top_k']\n",
    "    name2id = kwargs['name2id']\n",
    "    \n",
    "    corpus_sentences = annoy_object_wrapper.embedding_object.corpus_sentences\n",
    "    corpus_embeddings = annoy_object_wrapper.embedding_object.corpus_embeddings\n",
    "    name_to_id = annoy_object_wrapper.embedding_object.name_to_id\n",
    "    annoy_index = annoy_object_wrapper.annoy_index\n",
    "\n",
    "    query_embedding = model.encode(str(keyword), device=device)\n",
    "\n",
    "    found_corpus_ids, scores = annoy_index.get_nns_by_vector(query_embedding, top_k, include_distances=True)\n",
    "    hits = []\n",
    "\n",
    "    for _id, score in zip(found_corpus_ids, scores):\n",
    "        # Cosine Distance is equivalent to Euclidean distance of normalized vectors = sqrt(2-2*cos(u, v))\n",
    "        # cosine_dist = sqrt(2-2*cos(u,v))\n",
    "        # Thus cos(u,v) = 1-(cosine_dist**2)/2\n",
    "        hits.append({'corpus_id': _id, 'score': 1 - ((score ** 2) / 2)})\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return_hits= []\n",
    "\n",
    "    for hit in hits:\n",
    "        name = corpus_sentences[hit['corpus_id']]\n",
    "        possible_id = name2id.get(corpus_sentences[hit['corpus_id']], [])\n",
    "        for _id in possible_id:\n",
    "            return_hits.append((name, _id))\n",
    "\n",
    "    return return_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'Protein Kinase C'\n",
    "generate_candidate_with_annoy(search_term, model=model, top_k=10, annoy_object_wrapper=annoy_object_wrapper, id2name=id2name, name2id=name2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
