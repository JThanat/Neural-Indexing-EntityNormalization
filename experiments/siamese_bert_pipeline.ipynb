{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese BERT Pipeline for Entity Normalization\n",
    "\n",
    "This pipeline contains an experiment of siamese bert model for protein name normalization.\n",
    "We use BioCreative Dataset as query set as explained in [NSEEN Paper](https://www.isi.edu/~ambite/papers/NSEEN__Neural_Semantic_Embedding_for_Entity_Normalization.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set your CUDA Visible Devices in case you need to work with multiple GPU\n",
    "\n",
    "We need to set cuda device here before we import any pytorch module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import logging\n",
    "import nltk\n",
    "import glob\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification, BertModel\n",
    "from transformers import AdamW\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, SentencesDataset, InputExample, losses, evaluation\n",
    "\n",
    "# Add relative uitls folder Path\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from utils.uniprot_loader import *\n",
    "from utils.annoy_helper import *\n",
    "from utils.biocreative_helper import *\n",
    "from utils.hard_negative import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default device to cuda if available\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 Finetune BioBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA Preparation\n",
    "\n",
    "In this step we will use a built-in function to read data from uniprot_data_prep.\n",
    "\n",
    "You can simply come up with your own dataset by providing train_data and dev_data as a list of triplet consisting of name1, name2 and score [name1, name2, score]\n",
    "\n",
    "name1: str\n",
    "name2: str\n",
    "score: float\n",
    "\n",
    "```\n",
    "[\n",
    "    ['IF2(mt)', 'MTIF2', 1],\n",
    "    ['FRDA', 'Frataxin intermediate form', 1],\n",
    "    ['GATL3', 'L-JAK', 0],\n",
    "]\n",
    "```\n",
    " \n",
    "Or you can also use `get_data_from_path` to prepare the data for you\n",
    "\n",
    "`get_data_from_path` expects the following files in the folder. Each files must contain name1, name2 and label column.\n",
    "- train.tsv\n",
    "- dev.tsv\n",
    "\n",
    "An example of the file is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(ROOT_DIR, 'data/temp/dev.tsv'), delimiter='\\t').sample(frac=0.1).to_csv('../data/dev.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(ROOT_DIR, 'data/temp/train.tsv'), delimiter='\\t').sample(frac=0.005).to_csv('../data/train.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data, dev_data = get_data_from_path(os.path.join(ROOT_DIR, 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p1train = set([x[0] for x in train_data])\n",
    "p2train = set([x[1] for x in train_data])\n",
    "p1dev = set([x[0] for x in dev_data])\n",
    "p2dev = set([x[1] for x in dev_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Train: {}\".format(len(p1train.union(p2train))))\n",
    "print(\"All Dev: {}\".format(len(p1dev.union(p2dev))))\n",
    "print(\"Seen in Train: {}\".format(len(p1dev.union(p2dev).intersection(p1train.union(p2train)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Seen Dev Data\n",
    "\n",
    "To make sure that there is no cheating in our training step, We will remove the previously seen dev data in training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainset = p1train.union(p2train)\n",
    "dev_data_cleaned = [data for data in dev_data if data[0] not in trainset and data[1] not in trainset]\n",
    "\n",
    "p1dev_clean = set([x[0] for x in dev_data_cleaned])\n",
    "p2dev_clean = set([x[1] for x in dev_data_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Train: {}\".format(len(p1train.union(p2train))))\n",
    "print(\"All Dev: {}\".format(len(p1dev_clean.union(p2dev_clean))))\n",
    "print(\"Seen in Train: {}\".format(len(p1dev_clean.union(p2dev_clean).intersection(p1train.union(p2train)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Training Heuristic\n",
    "\n",
    "To help the model learn syntactic similarity between two name, we add a function to generate heuristic from the name to help it learn. One of heuristic examples is all lower case name vs Upper case for the first letter name. In this case, we gave a score of 0.9 to the name pair. The format of the data point must be a list or tuple of [name1, name2, score].\n",
    "\n",
    "__Example:__\n",
    "\n",
    "- [\"Aspirin\", \"aspirin\", 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_heuristic_1 = generate_heuristic(p1train.union(p2train))\n",
    "dev_heuristic_1 = generate_heuristic(p1dev_clean.union(p2dev_clean))\n",
    "\n",
    "train_data = [[s1,s2, float(score)] for s1,s2,score in train_data]\n",
    "dev_data_cleaned = [[s1,s2,float(score)] for s1,s2,score in dev_data_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_data_with_extra = train_data + train_heuristic_1\n",
    "dev_data_cleaned_with_extra = dev_data_cleaned + dev_heuristic_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model\n",
    "\n",
    "In this step we will load our saved pretrained model. The default model is `dmislab_biobert_v1.1` however you can supply any pretrained model into `get_model`\n",
    "\n",
    "```\n",
    "model = get_model('siamese-biobert-v1-1-ep-5-Dec-15-2020-with-heuris', device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training and Dev Data into dataloader\n",
    "\n",
    "It is important that we put the data into DataLoader so that the data get batched and shuffled properly during the training time. In this example, we create training dataset using InputExample provided by Sentence Transformer Library. This is similar to DataLoader in Pytorch.\n",
    "\n",
    "We will also supply dev data set into evaluator object so that the model can always evaluate the the performance during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = get_train_dataloader(model, train_data_with_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluator = get_dev_dataloader(model, dev_data_cleaned_with_extra, evaltype='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "total_iter = num_epochs * len(train_dataloader)\n",
    "evaluation_steps = len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use CosineSimilarityLoss. However, we can always change the loss function of our modeltraining here. Some useful losses can be founded here https://www.sbert.net/docs/package_reference/losses.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=evaluation_steps,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"FINISH TRAINING ... SAVING NOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "saved_model = os.path.join(ROOT_DIR, 'trained-model/siamese-biobert-v1-1-ep-{}-{}-with-heuris-2'.format(num_epochs, today.strftime(\"%b-%d-%Y\")))\n",
    "model.save(saved_model)\n",
    "print(\"FINISH SAVING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Prepare dataset for Semantic Search with Annoy\n",
    "\n",
    "In the previous step, we finetune our siamese biobert model to let the model learn a good embedding for protein name. Since linear scan take up to O(N) time to get us the closest name pair so it's impractical to use it to search the similar name in realtime. In this experiment, we use [Annoy](https://github.com/spotify/annoy) to get the approximate nearest neightbor of the query term (query vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will embed all HUMAN proteins into Annoy File so that we can do the fast look up.\n",
    "However, any input data with [\"id\", \"name\"] columns should also work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REF_DATA_PATH =  os.path.join(ROOT_DIR, 'data/reference_data.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(REF_DATA_PATH, delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "finetune_biobert = get_model(os.path.join(ROOT_DIR, \"trained-model/siamese-biobert-v1-1-ep-1-Nov-28-2021-with-heuris-2\"), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_object = AnnoyObjectWrapper(index_path='./mesh-embedding-4096-trees.ann', \n",
    "                                  embedding_path='./mesh-768-embedding.pkl', \n",
    "                                  reference_dataset_path=REF_DATA_PATH, \n",
    "                                  name2id_path='./mesh-name2id-embedding-size-1500000', \n",
    "                                  model=finetune_biobert, n_trees=4096, embedding_size=768, max_corpus_size=1500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annoy_object.create_embedding_and_index(create_new_embedding=True, create_new_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Hard Negative Mining\n",
    "\n",
    "Finetuning the model with random negative may not be ideal for the model to learn how to distinguish between actual synonym and the name which only look similar to the query term. Therefore, we introduce hard negative mining in this step so that we can help improving the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def list_contains_no_answer_in_higher_rank(lowest_correct_answer, hits, possible_answer, corpus_sentences):\n",
    "    for i in range(lowest_correct_answer, -1, -1):\n",
    "        hitname = corpus_sentences[hits[i]['corpus_id']]\n",
    "        if hitname not in possible_answer:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id2name(dataset_path):\n",
    "    all_name = pd.read_csv(dataset_path, delimiter='\\t')\n",
    "    all_name = all_name.dropna() \n",
    "    \n",
    "    id2name = {}\n",
    "    for idx, row in all_name.iterrows():\n",
    "        if row['id'].strip() not in id2name:\n",
    "            id2name[row['id']] = set()\n",
    "        id2name[row['id']].add(row['name'].strip())\n",
    "        \n",
    "    return id2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name2id(dataset_path):\n",
    "    all_name = pd.read_csv(dataset_path, delimiter='\\t')\n",
    "    all_name = all_name.dropna() \n",
    "    \n",
    "    name2id = {}\n",
    "    for idx, row in all_name.iterrows():\n",
    "        if row['name'].strip() not in name2id:\n",
    "            name2id[row['name']] = set()\n",
    "        name2id[row['name']].add(row['id'].strip())\n",
    "        \n",
    "    return name2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_hard_neg(hard_neg_path, last_k_files=None):\n",
    "    df_list = []\n",
    "    filenames = glob.glob(hard_neg_path + '/*')\n",
    "    filenames.sort(key=os.path.getmtime)\n",
    "    if last_k_files:\n",
    "        filenames = filenames[-last_k_files:]\n",
    "    print(\"reading hard_neg from filenames: \", filenames)    \n",
    "    for file in filenames:\n",
    "        df_list.append(pd.read_csv(file, delimiter='\\t'))\n",
    "    try:\n",
    "        df = pd.concat(df_list)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        df = pd.DataFrame(columns = ['name1', 'name2'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def hard_neg_mining_from_ref(model, annoy_object_wrapper, ref, id2name, top_k_hits=100):\n",
    "    corpus_sentences = annoy_object_wrapper.embedding_object.corpus_sentences\n",
    "    corpus_embeddings = annoy_object_wrapper.embedding_object.corpus_embeddings\n",
    "    name_to_id = annoy_object_wrapper.embedding_object.name_to_id\n",
    "    annoy_index = annoy_object_wrapper.annoy_index\n",
    "\n",
    "    hard_negative_pairs = []\n",
    "    from tqdm import tqdm\n",
    "    pbar = tqdm(total=len(ref), position=0, leave=True)\n",
    "    for i, kv in enumerate(ref):\n",
    "        query, answers = kv\n",
    "        query_embedding = model.encode(query)\n",
    "\n",
    "        found_corpus_ids, scores = annoy_index.get_nns_by_vector(query_embedding, top_k_hits, include_distances=True)\n",
    "        hits = []\n",
    "        for id, score in zip(found_corpus_ids, scores):\n",
    "            hits.append({'corpus_id': id, 'score': 1-((score**2) / 2)})\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        possible_answer = set()\n",
    "        for ans in answers:\n",
    "            possible_answer = possible_answer.union(id2name.get(ans, set()))\n",
    "            \n",
    "        # Get lowest top_k_hits\n",
    "        lowest_rank = top_k_hits - 1\n",
    "        lowest_hit = \"None\"\n",
    "        for i, hit in enumerate(hits[0:top_k_hits]):\n",
    "            hitname = corpus_sentences[hit['corpus_id']]\n",
    "            if hitname in possible_answer:\n",
    "                lowest_rank = i\n",
    "        hard_neg_count = 0\n",
    "        has_wrong_answer_above = list_contains_no_answer_in_higher_rank(lowest_rank, hits[0:top_k_hits], possible_answer, corpus_sentences)\n",
    "        for i, hit in enumerate(hits[0:top_k_hits]):\n",
    "            hitname = corpus_sentences[hit['corpus_id']]\n",
    "            if hitname not in possible_answer:\n",
    "                if has_wrong_answer_above:\n",
    "                    if i < lowest_rank:\n",
    "                        hard_negative_pairs.append([query, hitname])\n",
    "\n",
    "        pbar.update(1)\n",
    "    return hard_negative_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "id2name = get_id2name(REF_DATA_PATH)\n",
    "name2id = get_name2id(REF_DATA_PATH)\n",
    "all_dev_name = set() # We add everything into reference and evaluate with another dataset. So, we leave all_dev_name empty\n",
    "ref_for_hardneg = [(name, id_set) for name, id_set in list(name2id.items()) if name not in all_dev_name]\n",
    "hard_negative_pairs = hard_neg_mining_from_ref(finetune_biobert, annoy_object, ref_for_hardneg, id2name)\n",
    "len(hard_negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "with open(os.path.join(ROOT_DIR, 'experiments/hard_neg/hard_neg_{}.tsv'.format(len(glob.glob(os.path.join(ROOT_DIR, 'experiments/hard_neg/*'))) + 1)), 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    writer.writerow(['name1', 'name2'])\n",
    "    for pair in hard_negative_pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune siamese model with HardNeg Mining\n",
    "\n",
    "In this step, we will combine everything we have and finetune the model with hard negative mining.\n",
    "First we will read from the current_pretrained which we want to do a hard negative mining for.\n",
    "The following section simply combine input reading, finetuning model, create hard negative mining names and evaluate with BioCreative Data\n",
    "\n",
    "**BioCreative Data**: BioCreative Data is a dataset which consists of annotated protein name from BioMedical Publication. We will treat this dataset as an test dataset as we have already used every human protein in uniprot for training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_epoch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cur_pretrained = os.path.join(ROOT_DIR, 'trained-model/siamese-biobert-v1-1-ep-1-Nov-28-2021-with-heuris-2')\n",
    "id2name = get_id2name(REF_DATA_PATH)\n",
    "name2id = get_name2id(REF_DATA_PATH)\n",
    "top_k_hits = 10\n",
    "for training_round in range(num_training_epoch):\n",
    "    logger.setLevel(logging.INFO)\n",
    "    train_data, dev_data = get_data_from_path(os.path.join(ROOT_DIR, 'data'))\n",
    "    p1train = set([x[0] for x in train_data])\n",
    "    p2train = set([x[1] for x in train_data])\n",
    "    p1dev = set([x[0] for x in dev_data])\n",
    "    p2dev = set([x[1] for x in dev_data])\n",
    "\n",
    "    # clean dev data\n",
    "    trainset = p1train.union(p2train)\n",
    "    dev_data_cleaned = [data for data in dev_data if data[0] not in trainset and data[1] not in trainset]\n",
    "\n",
    "    train_heuristic_1 = generate_heuristic(p1train.union(p2train))\n",
    "    dev_heuristic_1 = generate_heuristic(p1dev_clean.union(p2dev_clean))\n",
    "\n",
    "    train_data = [[s1,s2, float(score)] for s1,s2,score in train_data]\n",
    "    dev_data_cleaned = [[s1,s2,float(score)] for s1,s2,score in dev_data_cleaned]\n",
    "\n",
    "    devname = set()\n",
    "    for n1, n2, s in dev_data_cleaned:\n",
    "        devname.add(n1)\n",
    "        devname.add(n2)\n",
    "\n",
    "    hard_neg_list = get_hard_neg(os.path.join(ROOT_DIR, 'experiments/hard_neg'))\n",
    "    hard_neg_train, hard_neg_dev = remove_dup_hard_neg(hard_neg_list, devname)\n",
    "\n",
    "    train_data_with_extra = train_data + train_heuristic_1 + hard_neg_train\n",
    "    dev_data_cleaned_with_extra = dev_data_cleaned + dev_heuristic_1 + hard_neg_dev\n",
    "\n",
    "    model = get_model(cur_pretrained, device=device)\n",
    "\n",
    "    # Create Train Dataset using InputExample Provided by Sentence Transformer Library\n",
    "    examples = []\n",
    "    batch_size = 64\n",
    "    for _, data in enumerate(train_data_with_extra):\n",
    "        s1, s2, score = data\n",
    "        ex = InputExample(texts=[s1,s2],label=score)\n",
    "        examples.append(ex)\n",
    "\n",
    "    train_dataset = SentencesDataset(examples, model)\n",
    "    train_dataloader = DataLoader(train_dataset[:100], shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    num_epochs = 1\n",
    "    total_iter = num_epochs * len(train_dataloader)\n",
    "\n",
    "    # Create Word Embedding model with max_seq_length of 256\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "    \n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs)\n",
    "\n",
    "    today = date.today()\n",
    "    saved_model = os.path.join(ROOT_DIR, 'trained-model/siamese-biobert-v1-1-ep-{}-{}-with-heuris-hard-neg-{}'.format(training_round, today.strftime(\"%b-%d-%Y\"), len(glob.glob(os.path.join(ROOT_DIR, 'experiments/hardneg/*')))))\n",
    "    model.save(saved_model)\n",
    "    print(\"FINISH SAVING\")\n",
    "\n",
    "    # Load Saved Model\n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "    finetune_biobert = get_model(saved_model, device=device)\n",
    "    annoy_object = AnnoyObjectWrapper(index_path='./mesh-embedding-4096-trees.ann', \n",
    "                                  embedding_path='./mesh-768-embedding.pkl', \n",
    "                                  reference_dataset_path=REF_DATA_PATH, \n",
    "                                  name2id_path='./mesh-name2id-embedding-size-1500000', \n",
    "                                  model=finetune_biobert, n_trees=4096, embedding_size=768, max_corpus_size=1500000)\n",
    "    annoy_object.create_embedding_and_index(create_new_embedding=True, create_new_index=True)\n",
    "    print(\"Getting HardNegMining\")\n",
    "    \n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "    ref_for_hardneg = [(name, id_set) for name, id_set in list(name2id.items())]\n",
    "    hard_negative_pairs = hard_neg_mining_from_ref(finetune_biobert, annoy_object, ref_for_hardneg, id2name)\n",
    "    \n",
    "    with open(os.path.join(ROOT_DIR, 'experiments/hard_neg/hard_neg_{}.tsv'.format(len(glob.glob(os.path.join(ROOT_DIR, 'experiments/hard_neg/*'))) + 1)), 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        writer.writerow(['name1', 'name2'])\n",
    "        for pair in hard_negative_pairs:\n",
    "            writer.writerow(pair)\n",
    "\n",
    "    cur_pretrained = saved_model\n",
    "    print(\"Finish Epoch \", training_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
